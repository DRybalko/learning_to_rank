{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'C:\\\\Users\\\\dmytro.rybalko\\\\Documents\\\\impementations\\Keras_PairCNN\\\\jacana-qa-naacl2013-data-results\\\\preprocessed_data\\\\'\n",
    "VOCAB_PATH = OUTPUT_PATH + 'vocab.json'\n",
    "EMBEDDING_PATH = OUTPUT_PATH + 'aquaint+wiki.txt.gz.ndim=50.bin'\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 500\n",
    "RANDOM_STATE =  42\n",
    "PATIENCE = 2\n",
    "ATOL = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmytro.rybalko\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''Model utils'''\n",
    "import os, sys\n",
    "\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "def load_embeddings(embedding_file, vocab):\n",
    "    \n",
    "    '''Load pre-learnt word embeddings.\n",
    "    Return: embedding: embedding matrix with dim |vocab| x dim\n",
    "            dim: dimension of the embeddings\n",
    "            rand_count: number of words not in trained embedding\n",
    "    '''\n",
    "    print('Loading word vectors...')\n",
    "    start = time.time()\n",
    "    outdir = OUTPUT_PATH\n",
    "\n",
    "    try:\n",
    "        print('Trying to load from npy dump.')\n",
    "        embedding = np.load(os.path.join(outdir, 'embedding.npy'))\n",
    "        return embedding, embedding.shape[1], 'NA'\n",
    "    except:\n",
    "        print('Load from dump failed, reading from binary.')\n",
    "\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\n",
    "        embedding_file, binary=True)\n",
    "    print('Loaded in %f seconds' %(time.time() - start))\n",
    "    # Need to use the word vectors to make embeddings matrix\n",
    "    # Get dimension for any word embedding\n",
    "    dim = word_vectors['apple'].shape[0]\n",
    "    \n",
    "    # Initialize an embedding of |vocab| x dim\n",
    "    # word -> embedding\n",
    "    embedding = np.zeros((len(vocab), dim))\n",
    "    # Take random values\n",
    "    rand_vec = np.random.uniform(-0.25, 0.25, dim)\n",
    "    # Count of words not having representations in our embedding file\n",
    "    rand_count = 0\n",
    "\n",
    "    for key, value in vocab.items():\n",
    "        # Map word idx to its embedding vector.\n",
    "        try:\n",
    "            embedding[value] = word_vectors[key]\n",
    "        except:\n",
    "            embedding[value] = rand_vec\n",
    "            rand_count += 1\n",
    "\n",
    "    print('Total time for loading embedding: %f seconds' %(time.time() - start))\n",
    "    print('Number of words not in trained embedding: %d' %(rand_count))\n",
    "\n",
    "    np.save(os.path.join(outdir, 'embedding.npy'), embedding)\n",
    "    return embedding, dim, rand_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmytro.rybalko\\Documents\\impementations\\Keras_PairCNN\\scripts\n"
     ]
    }
   ],
   "source": [
    "%cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_metrics as em\n",
    "\n",
    "def get_metrics(qids, y_true, y_pred, text):\n",
    "    \n",
    "    #train_acc = sklearn.metrics.roc_auc_score(y_train, y_pred)\n",
    "    map_score = em.map_score(qids, y_true, y_pred)\n",
    "    mrr_score = em.mrr_score(qids, y_true, y_pred)\n",
    "    print(text + '   MAP: %f, MRR: %f' %(map_score, mrr_score))\n",
    "    return map_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmytro.rybalko\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Main file to run the setup.'''\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import sys\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import bm25\n",
    "from model import cnn_model\n",
    "from utils import batch_gen\n",
    "import json\n",
    "\n",
    "# import tqdm\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def load_json(file_path):\n",
    "    return json.load(open(file_path, 'r'))\n",
    "        \n",
    "def generate_weights(qids, questions, answers, labels, lagrange_mult = 0.9, variance = 0.001):\n",
    "    print('Variance is', variance)\n",
    "    probs, relevances = bm25.predict_relevances(qids, questions, answers)\n",
    "    delta = (1 + np.array(labels)) * np.random.normal(loc = labels - relevances,\n",
    "                                                     scale = variance,\n",
    "                                                     size = np.array(qids).shape[0])\n",
    "    delta_langrange = np.power(delta, 2) - lagrange_mult\n",
    "    control_policy_weights = np.clip(delta/probs, -1000, 1000)\n",
    "    return control_policy_weights, relevances\n",
    "\n",
    "def train_model(mode):\n",
    "    '''Train the model.\n",
    "    1. Read numpy arrays for input data\n",
    "    2. Batch train the model\n",
    "    3. Calculate map scores using our method.\n",
    "    4. Dump predicted values in csv format for evaluation using Trec-eval\n",
    "    '''\n",
    "    if mode not in ['TRAIN-ALL', 'TRAIN']:\n",
    "        print('Invalid mode')\n",
    "        return\n",
    "\n",
    "    data_dir = os.path.join(OUTPUT_PATH, mode+'2')\n",
    "\n",
    "    # Load train set.\n",
    "    q_train = np.load(os.path.join(data_dir, '%s.questions.npy' %(mode.lower())))\n",
    "    a_train = np.load(os.path.join(data_dir, '%s.answers.npy' %(mode.lower())))\n",
    "    y_train = np.load(os.path.join(data_dir, '%s.labels.npy' %(mode.lower())))\n",
    "    qids_train = np.load(os.path.join(data_dir, '%s.qids.npy' %(mode.lower())))\n",
    "    addn_feat_train = np.zeros(y_train.shape)\n",
    "    \n",
    "    #weights, relevances = generate_weights(qids_train, q_train, a_train, y_train, variance = 0.000001)\n",
    "    \n",
    "    print('''q_train.shape, a_train.shape, y_train.shape, qids_train.shape,\n",
    "             addn_feat_train.shape: ''')\n",
    "    print(q_train.shape, q_train.shape, y_train.shape, qids_train.shape,\n",
    "          addn_feat_train.shape)\n",
    "\n",
    "    # Load dev and test sets.\n",
    "    q_dev = np.load(os.path.join(data_dir, 'dev.questions.npy'))\n",
    "    a_dev = np.load(os.path.join(data_dir, 'dev.answers.npy'))\n",
    "    y_dev = np.load(os.path.join(data_dir, 'dev.labels.npy'))\n",
    "    qids_dev = np.load(os.path.join(data_dir, 'dev.qids.npy'))\n",
    "    addn_feat_dev = np.zeros(y_dev.shape)\n",
    "\n",
    "    q_test = np.load(os.path.join(data_dir, 'test.questions.npy'))\n",
    "    a_test = np.load(os.path.join(data_dir, 'test.answers.npy'))\n",
    "    y_test = np.load(os.path.join(data_dir, 'test.labels.npy'))\n",
    "    qids_test = np.load(os.path.join(data_dir, 'test.qids.npy'))\n",
    "    addn_feat_test = np.zeros(y_test.shape)\n",
    "\n",
    "    vocab = load_json(VOCAB_PATH)\n",
    "    \n",
    "    max_ques_len = q_train.shape[1]\n",
    "    max_ans_len = a_train.shape[1]\n",
    "    embedding, embed_dim, _ = load_embeddings(EMBEDDING_PATH, vocab)\n",
    "\n",
    "    addit_feat_len = 1\n",
    "    if addn_feat_train.ndim > 1:\n",
    "        addit_feat_len = addn_feat_train.shape[1]\n",
    "\n",
    "    # Get model\n",
    "    cnn_model_instance = cnn_model(embed_dim, max_ques_len, max_ans_len,\n",
    "                                len(vocab), embedding,\n",
    "                                addit_feat_len=addit_feat_len)\n",
    "    \n",
    "    bs = BATCH_SIZE\n",
    "    np.set_printoptions(threshold=np.nan)\n",
    "    # np.seterr(divide='ignore', invalid='ignore')\n",
    "    # Train manually, epoch by epoch\n",
    "    # TODO: Add tqdm\n",
    "    log_path = './logs'\n",
    "    callback = TensorBoard(log_path)\n",
    "    callback.set_model(cnn_model_instance)\n",
    "    train_names = ['train_loss', 'train_acc']\n",
    "    dev_names = ['dev_loss', 'dev_acc']\n",
    "\n",
    "    y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "    get_metrics(qids_train, y_train, y_pred_train, 'Train initial ')\n",
    "    y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "    map_overall, mrr_dev = get_metrics(qids_dev, y_dev, y_pred_dev, 'Validation initial ')\n",
    "    y_pred_test = cnn_model_instance.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "    get_metrics(qids_test, y_test, y_pred_test, 'Test initial ')\n",
    "    \n",
    "    patience = 1\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        q_train, a_train, y_train, addn_feat_train = sklearn.utils.shuffle(\n",
    "            q_train, a_train, y_train, addn_feat_train,\n",
    "            random_state = RANDOM_STATE)\n",
    "          \n",
    "        batch_no = 0\n",
    "        \n",
    "        weights, relevances = generate_weights(qids_train, q_train, a_train, y_train, variance = 1e-6)\n",
    "        \n",
    "        for b_q_train, b_a_train, b_y_train, b_addn_feat_train, b_weights, b_qids_train, b_relevances in zip(\n",
    "                batch_gen(q_train, bs), batch_gen(a_train, bs),\n",
    "                batch_gen(y_train, bs), batch_gen(addn_feat_train, bs),\n",
    "                batch_gen(weights, bs), batch_gen(qids_train, bs),\n",
    "                batch_gen(relevances, bs)):\n",
    "        \n",
    "            loss_current = cnn_model_instance.train_on_batch(\n",
    "                [b_q_train, b_a_train, b_addn_feat_train, b_weights], b_relevances)\n",
    "            \n",
    "            \n",
    "            if batch_no%100 == 0:\n",
    "                #write_log(callback, train_names, logs, batch_no*(epoch+1))\n",
    "                \n",
    "                y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "                get_metrics(qids_train, y_train, y_pred_train, 'Batch {} train: '.format(batch_no))\n",
    "                y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "                map_current, mrr_dev = get_metrics(qids_dev, y_dev, y_pred_dev, 'Batch {} validation: '.format(batch_no))\n",
    "              \n",
    "            batch_no += 1\n",
    "        \n",
    "        y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "        get_metrics(qids_train, y_train, y_pred_train, 'Epoch {} train: '.format(epoch))\n",
    "        y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "        get_metrics(qids_dev, y_dev, y_pred_dev, 'Epoch {} validation: '.format(epoch))\n",
    "        \n",
    "        print('Loss is ', loss_current)\n",
    "        if np.abs(map_overall - map_current) <= ATOL:\n",
    "            if patience < PATIENCE:\n",
    "                patience += 1\n",
    "            else:\n",
    "                break\n",
    "        map_overall = map_current\n",
    "        \n",
    "    y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "    get_metrics(qids_train, y_train, y_pred_train, 'Train final ')\n",
    "    y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "    map_dev, mrr_dev = get_metrics(qids_dev, y_dev, y_pred_dev, 'Validation final ')\n",
    "    y_pred_test = cnn_model_instance.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "    get_metrics(qids_test, y_test, y_pred_test, 'Test final ')\n",
    "    \n",
    "    \"\"\"\n",
    "    # Dump data for trec eval\n",
    "    N = len(y_pred_test)\n",
    "    nnet_outdir = OUTPUT_PATH + 'output\\\\'\n",
    "\n",
    "    df_submission = pd.DataFrame(index=np.arange(N), columns=['qid', 'iter', 'docno', 'rank', 'sim', 'run_id'])\n",
    "    df_submission['qid'] = qids_test\n",
    "    df_submission['iter'] = 0\n",
    "    df_submission['docno'] = np.arange(N)\n",
    "    df_submission['rank'] = 0\n",
    "    df_submission['sim'] = y_pred\n",
    "    df_submission['run_id'] = 'nnet'\n",
    "    df_submission.to_csv(os.path.join(nnet_outdir, 'submission.txt'), header=False, index=False, sep=' ')\n",
    "\n",
    "    df_gold = pd.DataFrame(index=np.arange(N), columns=['qid', 'iter', 'docno', 'rel'])\n",
    "    df_gold['qid'] = qids_test\n",
    "    df_gold['iter'] = 0\n",
    "    df_gold['docno'] = np.arange(N)\n",
    "    df_gold['rel'] = y_test\n",
    "    df_gold.to_csv(os.path.join(nnet_outdir, 'gold.txt'), header=False, index=False, sep=' ')\n",
    "\n",
    "    #subprocess.call(\"/bin/sh eval/run_eval.sh '{}'\".format(nnet_outdir), shell=True)\n",
    "    return cnn_model_instance, models\n",
    "    \"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_train.shape, a_train.shape, y_train.shape, qids_train.shape,\n",
      "             addn_feat_train.shape: \n",
      "(4619, 33) (4619, 33) (4619,) (4619,) (4619,)\n",
      "Loading word vectors...\n",
      "Trying to load from npy dump.\n",
      "Preparing model with the following parameters: \n",
      "embed_dim, max_ques_len, max_ans_len, vocab_size, embedding,\n",
      "              addit_feat_len, no_conv_filters: \n",
      "50 33 40 52051 (52051, 50) 1 100\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ques_input (InputLayer)         (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 50)       2602550     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 33, 50)       2602550     ques_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 40, 100)      25100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ques_conv (Conv1D)              (None, 33, 100)      25100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ques_pool (GlobalMaxPooling1D)  (None, 100)          0           ques_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10000       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           ques_pool[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_addn_feat (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 202)          0           ques_pool[0][0]                  \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 input_addn_feat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 202)          41006       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 202)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            203         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,306,509\n",
      "Trainable params: 101,409\n",
      "Non-trainable params: 5,205,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train initial    MAP: 0.233073, MRR: 0.275632\n",
      "Validation initial    MAP: 0.392626, MRR: 0.425830\n",
      "Test initial    MAP: 0.355811, MRR: 0.381702\n",
      "Epoch: 0\n",
      "Variance is 1e-06\n",
      "Batch 0 train:    MAP: 0.208402, MRR: 0.329448\n",
      "Batch 0 validation:    MAP: 0.557613, MRR: 0.590278\n",
      "Epoch 0 train:    MAP: 0.149105, MRR: 0.172706\n",
      "Epoch 0 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Loss is  -2.2210276\n",
      "Epoch: 1\n",
      "Variance is 1e-06\n",
      "Batch 0 train:    MAP: 0.122707, MRR: 0.145561\n",
      "Batch 0 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Epoch 1 train:    MAP: 0.122707, MRR: 0.145561\n",
      "Epoch 1 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Loss is  -0.5729995\n",
      "Epoch: 2\n",
      "Variance is 1e-06\n",
      "Batch 0 train:    MAP: 0.135497, MRR: 0.185084\n",
      "Batch 0 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Epoch 2 train:    MAP: 0.135497, MRR: 0.185084\n",
      "Epoch 2 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Loss is  -0.37675765\n",
      "Epoch: 3\n",
      "Variance is 1e-06\n",
      "Batch 0 train:    MAP: 0.173284, MRR: 0.234561\n",
      "Batch 0 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Epoch 3 train:    MAP: 0.173284, MRR: 0.234561\n",
      "Epoch 3 validation:    MAP: 0.999999, MRR: 1.000000\n",
      "Loss is  -0.93893665\n",
      "Train final    MAP: 0.173284, MRR: 0.234561\n",
      "Validation final    MAP: 0.999999, MRR: 1.000000\n",
      "Test final    MAP: 1.000000, MRR: 1.000000\n"
     ]
    }
   ],
   "source": [
    "model_train = train_model(mode = 'TRAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_train_all = train_model(mode = 'TRAIN-ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(OUTPUT_PATH, 'TRAIN')\n",
    "q_test = np.load(os.path.join(data_dir, 'train.questions.npy'))\n",
    "a_test = np.load(os.path.join(data_dir, 'train.answers.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'train.labels.npy'))\n",
    "qids_test = np.load(os.path.join(data_dir, 'train.qids.npy'))\n",
    "addn_feat_test = np.zeros(y_test.shape)\n",
    "y_pred = mode_with_batch.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "test_acc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "test_map = map_score(qids_test, y_test, y_pred)\n",
    "test_mrr = mrr_score(qids_test, y_test, y_pred)\n",
    "print('Test AUC: %f, MAP: %f, MRR: %f' %(test_acc, test_map, test_mrr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'TRAIN-ALL'\n",
    "data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "q_train = np.load(os.path.join(data_dir, '%s.questions.npy' %(mode.lower())))\n",
    "a_train = np.load(os.path.join(data_dir, '%s.answers.npy' %(mode.lower())))\n",
    "y_train = np.load(os.path.join(data_dir, '%s.labels.npy' %(mode.lower())))\n",
    "qids_train = np.load(os.path.join(data_dir, '%s.qids.npy' %(mode.lower())))\n",
    "    \n",
    "variance = 0.01\n",
    "    \n",
    "probs, relevances = bm25.predict_relevances(qids_train, q_train, a_train)\n",
    "loss = (1 + np.array(y_train)) * np.random.normal(loc = y_train - relevances,\n",
    "                                                    scale = variance,\n",
    "                                                    size = np.array(qids_train).shape[0])\n",
    "loss = np.power(loss, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.map_score(qids_train, y_train, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.mrr_score(qids_train, y_train, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing no relevance queries\n",
    "Should be cleaned up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'TRAIN-ALL'\n",
    "data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "dataset = 'dev'\n",
    "q_test = np.load(os.path.join(data_dir, dataset + '.questions.npy'))\n",
    "a_test = np.load(os.path.join(data_dir, dataset + '.answers.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, dataset + '.labels.npy'))\n",
    "qids_test = np.load(os.path.join(data_dir, dataset + '.qids.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_labels = []\n",
    "for i in np.unique(qids_test):\n",
    "    mask = qids_test == i\n",
    "    labels = y_test[mask]\n",
    "    if (labels.sum() == 0) or (labels.sum() == labels.shape[0]):\n",
    "        bad_labels += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x not in bad_labels\n",
    "\n",
    "f = np.vectorize(f)  # or use a different name if you want to keep the original f\n",
    "\n",
    "mask = f(qids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"C:\\\\Users\\\\dmytro.rybalko\\\\Documents\\\\impementations\\Keras_PairCNN\\\\jacana-qa-naacl2013-data-results\\\\preprocessed_data\\\\TRAIN-ALL2\\\\\"\n",
    "np.save(os.path.join(outdir, 'dev.questions.npy'), q_test[mask])\n",
    "np.save(os.path.join(outdir, 'dev.answers.npy'), a_test[mask])\n",
    "np.save(os.path.join(outdir, 'dev.labels.npy'), y_test[mask])\n",
    "np.save(os.path.join(outdir, 'dev.qids.npy'), qids_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'TRAIN'\n",
    "data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "dataset = 'train'\n",
    "q_test = np.load(os.path.join(data_dir, dataset + '.questions.npy'))\n",
    "a_test = np.load(os.path.join(data_dir, dataset + '.answers.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, dataset + '.labels.npy'))\n",
    "qids_test = np.load(os.path.join(data_dir, dataset + '.qids.npy'))\n",
    "    \n",
    "variance = 0.10\n",
    "    \n",
    "probs, relevances = bm25.predict_relevances(qids_test, q_test, a_test, top_k = 3)\n",
    "loss = (1 + np.array(y_test)) * np.random.normal(loc = y_test - probs,\n",
    "                                                    scale = variance,\n",
    "                                                    size = np.array(qids_test).shape[0])\n",
    "loss = np.power(loss, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.map_score(qids_test, y_test, relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.mrr_score(qids_test, y_test, relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_avg(qids_test, y_test, probs):\n",
    "    scores = 0\n",
    "    count = 0\n",
    "    for i in np.unique(qids_test):\n",
    "        weights = np.array(i == qids_test)\n",
    "        if (sum(y_test[weights]) == 0 or sum(y_test[weights]) == y_test[weights].shape[0]): continue\n",
    "        score = sklearn.metrics.roc_auc_score(y_test[weights], probs[weights])\n",
    "        scores += score\n",
    "        count += 1\n",
    "    return scores/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_avg(qids_test, y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_avg(qids_test, y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_avg(qids_test, y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
