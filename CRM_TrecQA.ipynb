{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmytro.rybalko\\Documents\\impementations\\Keras_PairCNN\\scripts\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'C:\\\\Users\\\\dmytro.rybalko\\\\Documents\\\\impementations\\Keras_PairCNN\\\\jacana-qa-naacl2013-data-results\\\\preprocessed_data\\\\'\n",
    "VOCAB_PATH = OUTPUT_PATH + 'vocab.json'\n",
    "EMBEDDING_PATH = OUTPUT_PATH + 'aquaint+wiki.txt.gz.ndim=50.bin'\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 10\n",
    "RANDOM_STATE =  42\n",
    "PATIENCE = 20\n",
    "ATOL = 0.01\n",
    "%cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation_metrics as em\n",
    "\n",
    "def roc_auc_score_avg(qids_test, y_test, probs):\n",
    "    scores = 0\n",
    "    count = 0\n",
    "    for i in np.unique(qids_test):\n",
    "        weights = np.array(i == qids_test)\n",
    "        if (sum(y_test[weights]) == 0 or sum(y_test[weights]) == y_test[weights].shape[0]): continue\n",
    "        score = sklearn.metrics.roc_auc_score(y_test[weights], probs[weights])\n",
    "        scores += score\n",
    "        count += 1\n",
    "    return scores/count\n",
    "\n",
    "def get_metrics(qids, y_true, y_pred, text):\n",
    "    \n",
    "    #train_acc = sklearn.metrics.roc_auc_score(y_train, y_pred)\n",
    "    map_score = em.map_score(qids, y_true, y_pred)\n",
    "    mrr_score = em.mrr_score(qids, y_true, y_pred)\n",
    "    roc_auc_score = roc_auc_score_avg(qids, y_true, y_pred)\n",
    "    print(text + '   MAP: %f, MRR: %f, AUC: %f' %(map_score, mrr_score, roc_auc_score))\n",
    "    return map_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmytro.rybalko\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\importlib\\_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\dmytro.rybalko\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''Main file to run the setup.'''\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import sys\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import bm25\n",
    "from model import cnn_model\n",
    "from utils import batch_gen, load_embeddings\n",
    "import json\n",
    "\n",
    "# import tqdm\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def load_json(file_path):\n",
    "    return json.load(open(file_path, 'r'))\n",
    "        \n",
    "def generate_weights(qids, questions, answers, labels, lagrange_mult = 0.9, variance = 0.001, epsilon = 1e-3):\n",
    "    print('Variance is', variance)\n",
    "    probs, relevances = bm25.predict_relevances(qids, questions, answers)\n",
    "    probs[relevances==0] = 1 - probs[relevances==0]\n",
    "    probs = np.clip(probs, epsilon, 1)\n",
    "    delta = (1 + np.array(labels)) * np.random.normal(loc = labels - relevances,\n",
    "                                                     scale = variance,\n",
    "                                                     size = np.array(qids).shape[0])\n",
    "    delta_langrange = np.power(delta, 2) - lagrange_mult\n",
    "    control_policy_weights = delta_langrange/probs\n",
    "    return control_policy_weights, relevances, probs\n",
    "\n",
    "def train_model(mode):\n",
    "    '''Train the model.\n",
    "    1. Read numpy arrays for input data\n",
    "    2. Batch train the model\n",
    "    3. Calculate map scores using our method.\n",
    "    4. Dump predicted values in csv format for evaluation using Trec-eval\n",
    "    '''\n",
    "    if mode not in ['TRAIN-ALL', 'TRAIN']:\n",
    "        print('Invalid mode')\n",
    "        return\n",
    "\n",
    "    data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "\n",
    "    # Load train set.\n",
    "    q_train = np.load(os.path.join(data_dir, '%s.questions.npy' %(mode.lower())))\n",
    "    a_train = np.load(os.path.join(data_dir, '%s.answers.npy' %(mode.lower())))\n",
    "    y_train = np.load(os.path.join(data_dir, '%s.labels.npy' %(mode.lower())))\n",
    "    qids_train = np.load(os.path.join(data_dir, '%s.qids.npy' %(mode.lower())))\n",
    "    addn_feat_train = np.zeros(y_train.shape)\n",
    "    \n",
    "    #weights, relevances = generate_weights(qids_train, q_train, a_train, y_train, variance = 0.000001)\n",
    "    \n",
    "    print('''q_train.shape, a_train.shape, y_train.shape, qids_train.shape,\n",
    "             addn_feat_train.shape: ''')\n",
    "    print(q_train.shape, q_train.shape, y_train.shape, qids_train.shape,\n",
    "          addn_feat_train.shape)\n",
    "\n",
    "    # Load dev and test sets.\n",
    "    q_dev = np.load(os.path.join(data_dir, 'dev.questions.npy'))\n",
    "    a_dev = np.load(os.path.join(data_dir, 'dev.answers.npy'))\n",
    "    y_dev = np.load(os.path.join(data_dir, 'dev.labels.npy'))\n",
    "    qids_dev = np.load(os.path.join(data_dir, 'dev.qids.npy'))\n",
    "    addn_feat_dev = np.zeros(y_dev.shape)\n",
    "\n",
    "    q_test = np.load(os.path.join(data_dir, 'test.questions.npy'))\n",
    "    a_test = np.load(os.path.join(data_dir, 'test.answers.npy'))\n",
    "    y_test = np.load(os.path.join(data_dir, 'test.labels.npy'))\n",
    "    qids_test = np.load(os.path.join(data_dir, 'test.qids.npy'))\n",
    "    addn_feat_test = np.zeros(y_test.shape)\n",
    "\n",
    "    vocab = load_json(VOCAB_PATH)\n",
    "    \n",
    "    max_ques_len = q_train.shape[1]\n",
    "    max_ans_len = a_train.shape[1]\n",
    "    embedding, embed_dim, _ = load_embeddings(EMBEDDING_PATH, OUTPUT_PATH, vocab)\n",
    "\n",
    "    addit_feat_len = 1\n",
    "    if addn_feat_train.ndim > 1:\n",
    "        addit_feat_len = addn_feat_train.shape[1]\n",
    "\n",
    "    # Get model\n",
    "    cnn_model_instance = cnn_model(embed_dim, max_ques_len, max_ans_len,\n",
    "                                len(vocab), embedding,\n",
    "                                addit_feat_len=addit_feat_len)\n",
    "    \n",
    "    bs = BATCH_SIZE\n",
    "    np.set_printoptions(threshold=np.nan)\n",
    "    # np.seterr(divide='ignore', invalid='ignore')\n",
    "    # Train manually, epoch by epoch\n",
    "    # TODO: Add tqdm\n",
    "    log_path = './logs'\n",
    "    callback = TensorBoard(log_path)\n",
    "    callback.set_model(cnn_model_instance)\n",
    "    train_names = ['train_loss', 'train_acc']\n",
    "    dev_names = ['dev_loss', 'dev_acc']\n",
    "\n",
    "    y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "    get_metrics(qids_train, y_train, y_pred_train, 'Train initial ')\n",
    "    y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "    map_overall, mrr_dev = get_metrics(qids_dev, y_dev, y_pred_dev, 'Validation initial ')\n",
    "    y_pred_test = cnn_model_instance.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "    get_metrics(qids_test, y_test, y_pred_test, 'Test initial ')\n",
    "    \n",
    "    patience = 1\n",
    "    best_model_weights = cnn_model_instance.get_weights()\n",
    "    \n",
    "    weights, relevances, probs = generate_weights(qids_train, q_train, a_train, y_train, lagrange_mult = 0.9,\n",
    "                                              variance = 0.01)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        #weights, relevances, probs = generate_weights(qids_train, q_train, a_train, y_train, lagrange_mult = 0.9,\n",
    "        #                                      variance = 0.01)\n",
    "            \n",
    "        q_train_rand, a_train_rand, y_train_rand, addn_feat_train_rand, weights_rand, relevances_rand = sklearn.utils.shuffle(\n",
    "            q_train, a_train, y_train, addn_feat_train, weights, relevances, random_state = RANDOM_STATE)\n",
    "          \n",
    "        batch_no = 0\n",
    "                \n",
    "        for b_q_train, b_a_train, b_y_train, b_addn_feat_train, b_weights, b_relevances in zip(\n",
    "                batch_gen(q_train_rand, bs), batch_gen(a_train_rand, bs),\n",
    "                batch_gen(y_train_rand, bs), batch_gen(addn_feat_train_rand, bs),\n",
    "                batch_gen(weights_rand, bs), batch_gen(relevances_rand, bs)):\n",
    "        \n",
    "            loss_current = cnn_model_instance.train_on_batch(\n",
    "                [b_q_train, b_a_train, b_addn_feat_train, b_weights], b_relevances)\n",
    "            \n",
    "            \n",
    "            if batch_no%100 == 0 and batch_no != 0:\n",
    "                #write_log(callback, train_names, logs, batch_no*(epoch+1))\n",
    "                \n",
    "                y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "                get_metrics(qids_train, y_train, y_pred_train, 'Batch {} train: '.format(batch_no))\n",
    "                y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "                get_metrics(qids_dev, y_dev, y_pred_dev, 'Batch {} validation: '.format(batch_no))\n",
    "              \n",
    "            batch_no += 1\n",
    "        \n",
    "        y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "        get_metrics(qids_train, y_train, y_pred_train, 'Epoch {} train: '.format(epoch))\n",
    "        y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "        map_current, mrr_current = get_metrics(qids_dev, y_dev, y_pred_dev, 'Epoch {} validation: '.format(epoch))\n",
    "        \n",
    "        print(y_pred_train[0:10])\n",
    "        \n",
    "        S_train = np.mean(y_pred_train/probs)\n",
    "        print('S is', S_train)\n",
    "\n",
    "        if map_current > map_overall:\n",
    "            map_overall = map_current\n",
    "            best_model_weights = cnn_model_instance.get_weights()\n",
    "        elif patience < PATIENCE:\n",
    "            patience += 1\n",
    "        else: break\n",
    "        \n",
    "        print('Loss is ', loss_current)\n",
    " \n",
    "    y_pred_train = cnn_model_instance.predict([q_train, a_train, addn_feat_train, np.ones(shape = len(q_train))])\n",
    "    get_metrics(qids_train, y_train, y_pred_train, 'Train final ')\n",
    "    y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "    map_dev, mrr_dev = get_metrics(qids_dev, y_dev, y_pred_dev, 'Validation final ')\n",
    "    y_pred_test = cnn_model_instance.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "    get_metrics(qids_test, y_test, y_pred_test, 'Test final ')\n",
    "\n",
    "    #cnn_model_instance.set_weights(best_model_weights)\n",
    "    return cnn_model_instance, weights, relevances\n",
    "    \n",
    "    \"\"\"\n",
    "    # Dump data for trec eval\n",
    "    N = len(y_pred_test)\n",
    "    nnet_outdir = OUTPUT_PATH + 'output\\\\'\n",
    "\n",
    "    df_submission = pd.DataFrame(index=np.arange(N), columns=['qid', 'iter', 'docno', 'rank', 'sim', 'run_id'])\n",
    "    df_submission['qid'] = qids_test\n",
    "    df_submission['iter'] = 0\n",
    "    df_submission['docno'] = np.arange(N)\n",
    "    df_submission['rank'] = 0\n",
    "    df_submission['sim'] = y_pred\n",
    "    df_submission['run_id'] = 'nnet'\n",
    "    df_submission.to_csv(os.path.join(nnet_outdir, 'submission.txt'), header=False, index=False, sep=' ')\n",
    "\n",
    "    df_gold = pd.DataFrame(index=np.arange(N), columns=['qid', 'iter', 'docno', 'rel'])\n",
    "    df_gold['qid'] = qids_test\n",
    "    df_gold['iter'] = 0\n",
    "    df_gold['docno'] = np.arange(N)\n",
    "    df_gold['rel'] = y_test\n",
    "    df_gold.to_csv(os.path.join(nnet_outdir, 'gold.txt'), header=False, index=False, sep=' ')\n",
    "\n",
    "    #subprocess.call(\"/bin/sh eval/run_eval.sh '{}'\".format(nnet_outdir), shell=True)\n",
    "    return cnn_model_instance, models\n",
    "    \"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_train.shape, a_train.shape, y_train.shape, qids_train.shape,\n",
      "             addn_feat_train.shape: \n",
      "(4718, 33) (4718, 33) (4718,) (4718,) (4718,)\n",
      "Loading word vectors...\n",
      "Trying to load from npy dump.\n",
      "Preparing model with the following parameters: \n",
      "embed_dim, max_ques_len, max_ans_len, vocab_size, embedding,\n",
      "              addit_feat_len, no_conv_filters: \n",
      "50 33 40 52051 (52051, 50) 1 100\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ques_input (InputLayer)         (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 50)       2602550     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 33, 50)       2602550     ques_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 40, 100)      25100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ques_conv (Conv1D)              (None, 33, 100)      25100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ques_pool (GlobalMaxPooling1D)  (None, 100)          0           ques_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10000       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           ques_pool[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_addn_feat (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 202)          0           ques_pool[0][0]                  \n",
      "                                                                 dot_1[0][0]                      \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 input_addn_feat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 202)          41006       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 202)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            203         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,306,509\n",
      "Trainable params: 101,409\n",
      "Non-trainable params: 5,205,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train initial    MAP: 0.284489, MRR: 0.294816, AUC: 0.495454\n",
      "Validation initial    MAP: 0.462844, MRR: 0.483307, AUC: 0.396016\n",
      "Test initial    MAP: 0.497025, MRR: 0.540768, AUC: 0.487956\n",
      "Variance is 0.01\n",
      "Epoch: 0\n",
      "Epoch 0 train:    MAP: 0.257619, MRR: 0.262317, AUC: 0.426527\n",
      "Epoch 0 validation:    MAP: 0.453829, MRR: 0.458726, AUC: 0.326337\n",
      "[[1.6807826e-06]\n",
      " [1.3788056e-07]\n",
      " [8.5122105e-09]\n",
      " [6.4195615e-07]\n",
      " [9.9048606e-08]\n",
      " [2.1138791e-07]\n",
      " [1.4566663e-06]\n",
      " [1.2902359e-06]\n",
      " [8.8081038e-08]\n",
      " [2.1435527e-07]]\n",
      "S is 2.7597052363737122e-05\n",
      "Loss is  -6.405019\n",
      "Epoch: 1\n",
      "Epoch 1 train:    MAP: 0.257484, MRR: 0.262285, AUC: 0.425509\n",
      "Epoch 1 validation:    MAP: 0.454500, MRR: 0.458656, AUC: 0.328746\n",
      "[[7.8084668e-07]\n",
      " [5.5632810e-08]\n",
      " [2.8896172e-09]\n",
      " [2.8197513e-07]\n",
      " [3.9175067e-08]\n",
      " [8.5848853e-08]\n",
      " [6.6005390e-07]\n",
      " [5.8348519e-07]\n",
      " [3.5262822e-08]\n",
      " [8.9300698e-08]]\n",
      "S is 1.8002581108145976e-05\n",
      "Loss is  -6.432681\n",
      "Epoch: 2\n",
      "Epoch 2 train:    MAP: 0.257606, MRR: 0.262385, AUC: 0.426285\n",
      "Epoch 2 validation:    MAP: 0.454656, MRR: 0.458656, AUC: 0.329165\n",
      "[[5.7250418e-07]\n",
      " [3.8461266e-08]\n",
      " [1.8843669e-09]\n",
      " [2.0180087e-07]\n",
      " [2.7358569e-08]\n",
      " [5.9757518e-08]\n",
      " [4.7657025e-07]\n",
      " [4.2458004e-07]\n",
      " [2.4775499e-08]\n",
      " [6.3457158e-08]]\n",
      "S is 1.492798552375503e-05\n",
      "Loss is  -6.434152\n",
      "Epoch: 3\n",
      "Epoch 3 train:    MAP: 0.258462, MRR: 0.264000, AUC: 0.428380\n",
      "Epoch 3 validation:    MAP: 0.454904, MRR: 0.458684, AUC: 0.329713\n",
      "[[2.4698224e-07]\n",
      " [1.4171880e-08]\n",
      " [5.7111632e-10]\n",
      " [8.2494239e-08]\n",
      " [9.7848130e-09]\n",
      " [2.2309999e-08]\n",
      " [2.0055190e-07]\n",
      " [1.7851427e-07]\n",
      " [8.9850971e-09]\n",
      " [2.4198160e-08]]\n",
      "S is 9.663392177693317e-06\n",
      "Loss is  -6.434603\n",
      "Epoch: 4\n",
      "Epoch 4 train:    MAP: 0.258467, MRR: 0.263959, AUC: 0.428398\n",
      "Epoch 4 validation:    MAP: 0.454914, MRR: 0.458693, AUC: 0.329954\n",
      "[[2.60557897e-07]\n",
      " [1.51911905e-08]\n",
      " [6.17883356e-10]\n",
      " [8.77120456e-08]\n",
      " [1.04459215e-08]\n",
      " [2.36786288e-08]\n",
      " [2.09985458e-07]\n",
      " [1.87977520e-07]\n",
      " [9.61250191e-09]\n",
      " [2.57652459e-08]]\n",
      "S is 9.8017411979017e-06\n",
      "Loss is  -6.436537\n",
      "Epoch: 5\n",
      "Epoch 5 train:    MAP: 0.258362, MRR: 0.264009, AUC: 0.427216\n",
      "Epoch 5 validation:    MAP: 0.454908, MRR: 0.458693, AUC: 0.329911\n",
      "[[2.7687221e-07]\n",
      " [1.6410876e-08]\n",
      " [6.7986877e-10]\n",
      " [9.4306834e-08]\n",
      " [1.1334010e-08]\n",
      " [2.5542512e-08]\n",
      " [2.2355776e-07]\n",
      " [2.0005372e-07]\n",
      " [1.0546581e-08]\n",
      " [2.7804727e-08]]\n",
      "S is 9.98006108740477e-06\n",
      "Loss is  -6.437335\n",
      "Epoch: 6\n",
      "Epoch 6 train:    MAP: 0.258507, MRR: 0.264076, AUC: 0.428214\n",
      "Epoch 6 validation:    MAP: 0.454477, MRR: 0.458697, AUC: 0.328927\n",
      "[[2.2688542e-07]\n",
      " [1.2861183e-08]\n",
      " [5.0799337e-10]\n",
      " [7.5709778e-08]\n",
      " [8.8315506e-09]\n",
      " [1.9895376e-08]\n",
      " [1.7918408e-07]\n",
      " [1.6126022e-07]\n",
      " [8.2291791e-09]\n",
      " [2.1985869e-08]]\n",
      "S is 8.910446233193122e-06\n",
      "Loss is  -6.438342\n",
      "Epoch: 7\n",
      "Epoch 7 train:    MAP: 0.258517, MRR: 0.264001, AUC: 0.428358\n",
      "Epoch 7 validation:    MAP: 0.454900, MRR: 0.458697, AUC: 0.329774\n",
      "[[3.4169432e-07]\n",
      " [2.1133564e-08]\n",
      " [9.1770685e-10]\n",
      " [1.1864044e-07]\n",
      " [1.4702731e-08]\n",
      " [3.2156745e-08]\n",
      " [2.7104022e-07]\n",
      " [2.4411924e-07]\n",
      " [1.3750168e-08]\n",
      " [3.5644000e-08]]\n",
      "S is 1.0568138982416726e-05\n",
      "Loss is  -6.439104\n",
      "Epoch: 8\n",
      "Epoch 8 train:    MAP: 0.258448, MRR: 0.264002, AUC: 0.428222\n",
      "Epoch 8 validation:    MAP: 0.454460, MRR: 0.458697, AUC: 0.328892\n",
      "[[2.5402281e-07]\n",
      " [1.4472799e-08]\n",
      " [5.8812943e-10]\n",
      " [8.4595619e-08]\n",
      " [1.0030045e-08]\n",
      " [2.2142631e-08]\n",
      " [1.9378538e-07]\n",
      " [1.7599271e-07]\n",
      " [9.3446264e-09]\n",
      " [2.4875646e-08]]\n",
      "S is 9.003444171067588e-06\n",
      "Loss is  -6.4401727\n",
      "Epoch: 9\n",
      "Epoch 9 train:    MAP: 0.258310, MRR: 0.264054, AUC: 0.427725\n",
      "Epoch 9 validation:    MAP: 0.454460, MRR: 0.458697, AUC: 0.328892\n",
      "[[2.3352430e-07]\n",
      " [1.2955677e-08]\n",
      " [5.1794546e-10]\n",
      " [7.6240582e-08]\n",
      " [8.9767207e-09]\n",
      " [1.9740774e-08]\n",
      " [1.7426711e-07]\n",
      " [1.5895358e-07]\n",
      " [8.3338714e-09]\n",
      " [2.2377080e-08]]\n",
      "S is 8.527106457790611e-06\n",
      "Loss is  -6.440815\n",
      "Train final    MAP: 0.258310, MRR: 0.264054, AUC: 0.427725\n",
      "Validation final    MAP: 0.454460, MRR: 0.458697, AUC: 0.328892\n",
      "Test final    MAP: 0.452095, MRR: 0.463927, AUC: 0.352458\n"
     ]
    }
   ],
   "source": [
    "best_model, weights, relevances = train_model(mode = 'TRAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(OUTPUT_PATH, 'TRAIN')\n",
    "dataset = 'train'\n",
    "q_test = np.load(os.path.join(data_dir, dataset + '.questions.npy'))\n",
    "a_test = np.load(os.path.join(data_dir, dataset + '.answers.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, dataset + '.labels.npy'))\n",
    "qids_test = np.load(os.path.join(data_dir, dataset + '.qids.npy'))\n",
    "addn_feat_test = np.zeros(y_test.shape)\n",
    "y_pred = best_model.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "map, mrr = get_metrics(qids_test, y_test, y_pred, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'TRAIN'\n",
    "dataset = 'train'\n",
    "data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "q_train = np.load(os.path.join(data_dir, '%s.questions.npy' %(dataset)))\n",
    "a_train = np.load(os.path.join(data_dir, '%s.answers.npy' %(dataset)))\n",
    "y_train = np.load(os.path.join(data_dir, '%s.labels.npy' %(dataset)))\n",
    "qids_train = np.load(os.path.join(data_dir, '%s.qids.npy' %(dataset)))\n",
    "    \n",
    "variance = 0.01\n",
    "    \n",
    "probs, relevances = bm25.predict_relevances(qids_train, q_train, a_train, top_k = 3)\n",
    "probs[relevances==0] = 1 - probs[relevances==0]\n",
    "probs = np.clip(probs, 1e-3, 1)\n",
    "loss = (1 + np.array(y_train)) * np.random.normal(loc = y_train - relevances,\n",
    "                                                    scale = 0.0001,\n",
    "                                                    size = np.array(qids_train).shape[0])\n",
    "loss = np.power(loss, 2) - 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_copy = y_pred.copy()\n",
    "y_pred_copy = y_pred_copy.reshape(-1)\n",
    "y_pred_copy[relevances==0] = 1 - y_pred_copy[relevances==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(weights * y_pred_copy) + 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss/probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(qids_train, y_train, probs, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = loss/probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(weights, range = (-10 , 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[qids_train == '21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevances[qids_train == '21']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing no relevance queries\n",
    "Should be cleaned up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'TRAIN-ALL'\n",
    "data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "dataset = 'dev'\n",
    "q_test = np.load(os.path.join(data_dir, dataset + '.questions.npy'))\n",
    "a_test = np.load(os.path.join(data_dir, dataset + '.answers.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, dataset + '.labels.npy'))\n",
    "qids_test = np.load(os.path.join(data_dir, dataset + '.qids.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_labels = []\n",
    "for i in np.unique(qids_test):\n",
    "    mask = qids_test == i\n",
    "    labels = y_test[mask]\n",
    "    if (labels.sum() == 0) or (labels.sum() == labels.shape[0]):\n",
    "        bad_labels += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x not in bad_labels\n",
    "\n",
    "f = np.vectorize(f)  # or use a different name if you want to keep the original f\n",
    "\n",
    "mask = f(qids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"C:\\\\Users\\\\dmytro.rybalko\\\\Documents\\\\impementations\\Keras_PairCNN\\\\jacana-qa-naacl2013-data-results\\\\preprocessed_data\\\\TRAIN-ALL2\\\\\"\n",
    "np.save(os.path.join(outdir, 'dev.questions.npy'), q_test[mask])\n",
    "np.save(os.path.join(outdir, 'dev.answers.npy'), a_test[mask])\n",
    "np.save(os.path.join(outdir, 'dev.labels.npy'), y_test[mask])\n",
    "np.save(os.path.join(outdir, 'dev.qids.npy'), qids_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'TRAIN'\n",
    "data_dir = os.path.join(OUTPUT_PATH, mode)\n",
    "dataset = 'train'\n",
    "q_test = np.load(os.path.join(data_dir, dataset + '.questions.npy'))\n",
    "a_test = np.load(os.path.join(data_dir, dataset + '.answers.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, dataset + '.labels.npy'))\n",
    "qids_test = np.load(os.path.join(data_dir, dataset + '.qids.npy'))\n",
    "    \n",
    "variance = 0.10\n",
    "    \n",
    "probs, relevances = bm25.predict_relevances(qids_test, q_test, a_test, top_k = 3)\n",
    "loss = (1 + np.array(y_test)) * np.random.normal(loc = y_test - probs,\n",
    "                                                    scale = variance,\n",
    "                                                    size = np.array(qids_test).shape[0])\n",
    "loss = np.power(loss, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.map_score(qids_test, y_test, relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.mrr_score(qids_test, y_test, relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_avg(qids_test, y_test, probs):\n",
    "    scores = 0\n",
    "    count = 0\n",
    "    for i in np.unique(qids_test):\n",
    "        weights = np.array(i == qids_test)\n",
    "        if (sum(y_test[weights]) == 0 or sum(y_test[weights]) == y_test[weights].shape[0]): continue\n",
    "        score = sklearn.metrics.roc_auc_score(y_test[weights], probs[weights])\n",
    "        scores += score\n",
    "        count += 1\n",
    "    return scores/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_avg(qids_test, y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_avg(qids_test, y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_avg(qids_test, y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
