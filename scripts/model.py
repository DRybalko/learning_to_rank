#Model implementation borrowed from https://github.com/gvishal/rank_text_cnn

import os, sys

import gensim
from gensim.models.keyedvectors import KeyedVectors

import keras
from keras import backend as K
from keras import optimizers, regularizers
from keras.layers import Dense, Dropout
from keras.layers import Input, Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D
from keras.layers.merge import Concatenate, Dot
from keras.layers import Multiply 
from keras.models import Sequential, Model

import numpy as np
import time

def counterfactual_risk_loss(weights):
    """
    Parameters
    ----------
    weights: np.array
        Weights generated by control policy and defined as (sigma - lagrange_mult)/control_propensity.
    """
    
    def compute_loss(y_true, y_pred):
        """
        Parameters
        ---------
        y_true: np.array
            Relevances (threshold probabilities) generated by bm25
        y_pred: np.array
            Softmax probability vector coming from the model
        """
        
        y_pred_prob = y_true*y_pred + (1 - y_true)*(1 - y_pred)
        return K.mean(weights * y_pred_prob, axis = -1)

    return compute_loss

def cnn_model(embed_dim, max_ques_len, max_ans_len, vocab_size, embedding,
              addit_feat_len, no_conv_filters=100):
    
    print('Preparing model with the following parameters: ')
    print('''embed_dim, max_ques_len, max_ans_len, vocab_size, embedding,
              addit_feat_len, no_conv_filters: ''')
    print(embed_dim, max_ques_len, max_ans_len, vocab_size, embedding.shape, 
          addit_feat_len, no_conv_filters)

    #Question layers
    input_q = Input(shape=(max_ques_len,), name='ques_input')
    
    #Load embedding values from corpus here.
    embed_q = Embedding(input_dim=vocab_size, output_dim=embed_dim,
                        input_length=max_ques_len,
                        weights=[embedding], trainable=False)(input_q)
    
    conv_q = Conv1D(filters=no_conv_filters, kernel_size=5, strides=1, padding='same',
                    activation='relu',
                    kernel_regularizer=regularizers.l2(1e-5),
                    name='ques_conv')(embed_q)
   
    pool_q = GlobalMaxPooling1D(name='ques_pool')(conv_q)

    # Answer Layers
    input_a = Input(shape=(max_ans_len,))
    
    embed_a = Embedding(input_dim=vocab_size, output_dim=embed_dim,
                        input_length=max_ans_len,
                        weights=[embedding], trainable=False)(input_a)
    conv_a = Conv1D(filters=no_conv_filters, kernel_size=5, strides=1, padding='same',
                    activation='relu',
                    kernel_regularizer=regularizers.l2(1e-5))(embed_a)
    pool_a = GlobalMaxPooling1D()(conv_a)

    # M or the similarity layer here
    # Paper: x_d_dash = M.x_d
    x_a = Dense(no_conv_filters, use_bias=False,
                kernel_regularizer=regularizers.l2(1e-4))(pool_a)
    sim = Dot(axes=-1)([pool_q, x_a])

    # Additional features input
    input_addn_feat = Input(shape=(addit_feat_len, ), name='input_addn_feat')
    
    # Combine Question, sim, Answer pooled outputs and additional input features
    join_layer = keras.layers.concatenate([pool_q, sim, pool_a,
                                            input_addn_feat])

    # hidden_units = join_layer.output_shape[1]
    hidden_units = no_conv_filters + 1 + no_conv_filters + addit_feat_len
    # Using relu here too? Not mentioned in the paper.
    hidden_layer = Dense(hidden_units,
                         kernel_regularizer=regularizers.l2(1e-4),
                         activation = 'relu')(join_layer)
    hidden_layer = Dropout(0.5)(hidden_layer)

    # Final Softmax Layer, add regularizer here too?
    softmax = Dense(1, activation='sigmoid')(hidden_layer)

    # Input to pass control propensity weights into the loss function
    input_weights = Input(shape=(1, ), name = 'input_weights')
    
    model = Model(inputs=[input_q, input_a, input_addn_feat, input_weights], outputs=softmax)
    print(model.summary())

    #adadelta = optimizers.Adadelta(rho=0.95, epsilon=1e-06)
    adam = optimizers.Adam(lr = 0.01)
    
    model.compile(optimizer=adam,
                  loss=counterfactual_risk_loss(input_weights))
                  #metrics=['accuracy'])

    return model